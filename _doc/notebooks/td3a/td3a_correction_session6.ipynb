{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 6 : Map/Reduce avec PIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan\n",
    "\n",
    "- [Exercice 1 : GROUP BY](#exo1)\n",
    "- [Exercice 2 : JOIN](#exo2)\n",
    "- [Prolongements](#prol)\n",
    "\n",
    "<div style=\"position:absolute; top:10px; right:5px; width:100px; margin:10px; background-color:#DDDDDD;\">\n",
    "[Exo 1 : GROUP BY](#exo1) -- [Exo 2 : JOIN](#exo2) -- [Prolongements](#prol)\n",
    "</div>\n",
    "\n",
    "On considère le jeu de données suivant : [Localization Data for Person Activity Data Set](https://archive.ics.uci.edu/ml/datasets/Localization+Data+for+Person+Activity) qu'on récupère comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background-color:gainsboro; padding:2px; border:0px;\"><b>server + credentials</b>\n",
       "<br />password <input type=\"password\" id=\"paramspassword\" value=\"\" size=\"80\" />\n",
       "<br />server <input type=\"text\" id=\"paramsserver\" value=\"\" size=\"80\" />\n",
       "<br />username <input type=\"text\" id=\"paramsusername\" value=\"dz...fr\" size=\"80\" />\n",
       "<br /><button onclick=\"set_valueparams()\">Ok</button></div>\n",
       "<script type=\"text/Javascript\">\n",
       "function set_valueparams(){\n",
       "   command='params = {' ;\n",
       "   var paramspasswordvar_value = document.getElementById('paramspassword').value;\n",
       "   command += '\"password\":\"' + paramspasswordvar_value + '\",';\n",
       "   var paramsservervar_value = document.getElementById('paramsserver').value;\n",
       "   command += '\"server\":\"' + paramsservervar_value + '\",';\n",
       "   var paramsusernamevar_value = document.getElementById('paramsusername').value;\n",
       "   command += '\"username\":\"' + paramsusernamevar_value + '\",';\n",
       "   command += '}';\n",
       "   var kernel = IPython.notebook.kernel;\n",
       "   kernel.execute(command);\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x7342a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyquickhelper, pyensae\n",
    "params={\"server\":\"\", \"username\":\"dz...fr\", \"password\":\"\"}\n",
    "pyquickhelper.open_html_form(params=params,title=\"server + credentials\", key_save=\"params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "password = params[\"password\"]\n",
    "server = params[\"server\"]\n",
    "username = params[\"username\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyensae.remote.remote_connection.ASSHClient at 0xa201a50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"exo1\">Exercice 1 : GROUP BY</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>      falling</td>\n",
       "      <td>  2973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>        lying</td>\n",
       "      <td> 54480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>   lying down</td>\n",
       "      <td>  6168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> on all fours</td>\n",
       "      <td>  5210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>      sitting</td>\n",
       "      <td> 27244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity     nb\n",
       "0       falling   2973\n",
       "1         lying  54480\n",
       "2    lying down   6168\n",
       "3  on all fours   5210\n",
       "4       sitting  27244"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas, sqlite3\n",
    "con = sqlite3.connect(\"ConfLongDemo_JSI.db3\")\n",
    "df = pandas.read_sql(\"\"\"SELECT activity, count(*) as nb FROM person GROUP BY activity\"\"\", con)\n",
    "con.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut maintenant le faire avec PIG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%PIG solution_groupby.pig\n",
    "\n",
    "myinput = LOAD 'ConfLongDemo_JSI.small.example.txt' \n",
    "          using PigStorage(',') \n",
    "          AS (index:long, sequence, tag, timestamp:long, dateformat, x:double,y:double, z:double, activity) ;\n",
    "\n",
    "gr = GROUP myinput BY activity ;\n",
    "avgact = FOREACH gr GENERATE group, COUNT(myinput) ; \n",
    "\n",
    "STORE avgact INTO 'ConfLongDemo_JSI.small.group.txt' USING PigStorage() ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x6fb96d8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%jobsubmit solution_groupby.pig groupby.redirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "Total bytes written : 89\n",
       "Spillable Memory Manager spill count : 0\n",
       "Total bags proactively spilled: 0\n",
       "Total records proactively spilled: 0\n",
       "\n",
       "Job DAG:\n",
       "job_1410345524631_0011\n",
       "\n",
       "\n",
       "2014-10-25 00:26:50,476 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x9139080>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd tail groupby.redirection.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "Found 2 items\n",
       "-rw-r--r--   3 xavierdupre xavierdupre          0 2014-10-25 00:26 ConfLongDemo_JSI.small.group.txt/_SUCCESS\n",
       "-rw-r--r--   3 xavierdupre xavierdupre         89 2014-10-25 00:26 ConfLongDemo_JSI.small.group.txt/part-r-00000\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x91390f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -ls ConfLongDemo_JSI.small.group.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "lying\t267\n",
       "falling\t30\n",
       "sitting\t435\n",
       "walking\t170\n",
       "sitting down\t56\n",
       "standing up from sitting\t42\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x9139128>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -tail ConfLongDemo_JSI.small.group.txt/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"exo2\">Exercice 2 : JOIN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sequence</th>\n",
       "      <th>tag</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>dateformat</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>activity</th>\n",
       "      <th>nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td> A01</td>\n",
       "      <td> 010-000-024-033</td>\n",
       "      <td> 633790226051280329</td>\n",
       "      <td> 27.05.2009 14:03:25:127</td>\n",
       "      <td> 4.062931</td>\n",
       "      <td> 1.892434</td>\n",
       "      <td> 0.507425</td>\n",
       "      <td> walking</td>\n",
       "      <td> 32710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td> A01</td>\n",
       "      <td> 020-000-033-111</td>\n",
       "      <td> 633790226051820913</td>\n",
       "      <td> 27.05.2009 14:03:25:183</td>\n",
       "      <td> 4.291954</td>\n",
       "      <td> 1.781140</td>\n",
       "      <td> 1.344495</td>\n",
       "      <td> walking</td>\n",
       "      <td> 32710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 2</td>\n",
       "      <td> A01</td>\n",
       "      <td> 020-000-032-221</td>\n",
       "      <td> 633790226052091205</td>\n",
       "      <td> 27.05.2009 14:03:25:210</td>\n",
       "      <td> 4.359101</td>\n",
       "      <td> 1.826456</td>\n",
       "      <td> 0.968821</td>\n",
       "      <td> walking</td>\n",
       "      <td> 32710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 3</td>\n",
       "      <td> A01</td>\n",
       "      <td> 010-000-024-033</td>\n",
       "      <td> 633790226052361498</td>\n",
       "      <td> 27.05.2009 14:03:25:237</td>\n",
       "      <td> 4.087835</td>\n",
       "      <td> 1.879999</td>\n",
       "      <td> 0.466983</td>\n",
       "      <td> walking</td>\n",
       "      <td> 32710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 4</td>\n",
       "      <td> A01</td>\n",
       "      <td> 010-000-030-096</td>\n",
       "      <td> 633790226052631792</td>\n",
       "      <td> 27.05.2009 14:03:25:263</td>\n",
       "      <td> 4.324462</td>\n",
       "      <td> 2.072460</td>\n",
       "      <td> 0.488065</td>\n",
       "      <td> walking</td>\n",
       "      <td> 32710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index sequence              tag           timestamp  \\\n",
       "0      0      A01  010-000-024-033  633790226051280329   \n",
       "1      1      A01  020-000-033-111  633790226051820913   \n",
       "2      2      A01  020-000-032-221  633790226052091205   \n",
       "3      3      A01  010-000-024-033  633790226052361498   \n",
       "4      4      A01  010-000-030-096  633790226052631792   \n",
       "\n",
       "                dateformat         x         y         z activity     nb  \n",
       "0  27.05.2009 14:03:25:127  4.062931  1.892434  0.507425  walking  32710  \n",
       "1  27.05.2009 14:03:25:183  4.291954  1.781140  1.344495  walking  32710  \n",
       "2  27.05.2009 14:03:25:210  4.359101  1.826456  0.968821  walking  32710  \n",
       "3  27.05.2009 14:03:25:237  4.087835  1.879999  0.466983  walking  32710  \n",
       "4  27.05.2009 14:03:25:263  4.324462  2.072460  0.488065  walking  32710  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = sqlite3.connect(\"ConfLongDemo_JSI.db3\")\n",
    "df = pandas.read_sql(\"\"\"SELECT person.*, A.nb FROM person INNER JOIN (\n",
    "                            SELECT activity, count(*) as nb FROM person GROUP BY activity) AS A\n",
    "                            ON person.activity == A.activity\"\"\", con)\n",
    "con.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem, maintenant il faut le faire avec PIG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%PIG solution_groupby_join.pig\n",
    "\n",
    "myinput = LOAD 'ConfLongDemo_JSI.small.example.txt' \n",
    "          using PigStorage(',') \n",
    "          AS (index:long, sequence, tag, timestamp:long, dateformat, x:double,y:double, z:double, activity) ;\n",
    "\n",
    "gr = GROUP myinput BY activity ;\n",
    "avgact = FOREACH gr GENERATE group, COUNT(myinput) ; \n",
    "\n",
    "joined = JOIN myinput BY activity, avgact BY group ;\n",
    "\n",
    "STORE joined INTO 'ConfLongDemo_JSI.small.group.join.txt' USING PigStorage() ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x6fb9748>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%jobsubmit solution_groupby_join.pig groupby.join.redirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "job_1410345524631_0012\t->\tjob_1410345524631_0013,\n",
       "job_1410345524631_0013\n",
       "\n",
       "\n",
       "2014-10-25 00:58:26,919 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: dn03.dzr323.dza.datazoomr.com/10.58.223.23:60910. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
       "2014-10-25 00:58:27,920 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: dn03.dzr323.dza.datazoomr.com/10.58.223.23:60910. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
       "2014-10-25 00:58:28,922 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: dn03.dzr323.dza.datazoomr.com/10.58.223.23:60910. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
       "2014-10-25 00:58:29,039 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
       "2014-10-25 00:58:29,254 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
       "2014-10-25 00:58:29,400 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x9139c50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd tail groupby.join.redirection.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "Found 2 items\n",
       "-rw-r--r--   3 xavierdupre xavierdupre          0 2014-10-25 00:58 ConfLongDemo_JSI.small.group.join.txt/_SUCCESS\n",
       "-rw-r--r--   3 xavierdupre xavierdupre     144059 2014-10-25 00:58 ConfLongDemo_JSI.small.group.join.txt/part-r-00000\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x91453c8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -ls ConfLongDemo_JSI.small.group.join.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "26262834000\t27.05.2009 14:03:46:283\t3.3038318157196045\t1.938292145729065\t0.7622964978218079\tstanding up from sitting\tstanding up from sitting\t42\n",
       "652\tA01\t020-000-033-111\t633790226262563704\t27.05.2009 14:03:46:257\t3.2363295555114746\t2.00623106956482\t1.1472841501235962\tstanding up from sitting\tstanding up from sitting\t42\n",
       "651\tA01\t010-000-030-096\t633790226262293413\t27.05.2009 14:03:46:230\t3.275949239730835\t1.7746492624282837\t0.3117055296897888\tstanding up from sitting\tstanding up from sitting\t42\n",
       "650\tA01\t010-000-024-033\t633790226262023117\t27.05.2009 14:03:46:203\t3.2498104572296143\t1.878917098045349\t0.13854867219924927\tstanding up from sitting\tstanding up from sitting\t42\n",
       "649\tA01\t020-000-032-221\t633790226261752823\t27.05.2009 14:03:46:177\t3.352446317672729\t1.950886845588684\t0.8281049728393555\tstanding up from sitting\tstanding up from sitting\t42\n",
       "648\tA01\t020-000-033-111\t633790226261482530\t27.05.2009 14:03:46:147\t3.2220029830932617\t2.0042579174041752\t1.032345414161682\tstanding up from sitting\tstanding up from sitting\t42\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x9145518>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -tail ConfLongDemo_JSI.small.group.join.txt/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"prol\">Prolongements</h3>\n",
    "\n",
    "[PIG](http://pig.apache.org/) n'est pas la seule façon d'exécuter des jobs Map/Reduce. [Hive](https://hive.apache.org/) est un langage dont la syntaxe est très proche de celle du SQL. L'article [Comparing Pig Latin and SQL for Constructing Data Processing Pipelines](https://developer.yahoo.com/blogs/hadoop/comparing-pig-latin-sql-constructing-data-processing-pipelines-444.html) explicite les différences des deux approches.\n",
    "\n",
    "**langage haut niveau**\n",
    "\n",
    "Ce qu'il faut retenir est que le langage PIG est un langage haut niveau. Le programme est compilé en une séquence d'opérations Map/Reduce transparente pour l'utilisateur. Le temps de développement est très réduit lorsqu'on le compare au même programme écrit en Java. Le compilateur construit un plan d'exécution ([quelques exemples ici](http://chimera.labs.oreilly.com/books/1234000001811/ch07.html#explain)) et infère le nombre de machines requises pour distribuer le job. Cela suffit pour la plupart des besoins, cela nécessite.\n",
    "\n",
    "**petits jeux**\n",
    "\n",
    "Certains jobs peuvent durer des heures, il est conseillée de les essayer sur des petits jeux de données avant de les faire tourner sur les vrais données. Il est toujours frustrant de s'apercevoir qu'un job a planté au bout de deux heures car une chaîne de caractères est vide et que ce cas n'a pas été prévu.\n",
    "\n",
    "Avec ces petits jeux, il est possible de faire tourner et conseillé de tester le job d'abord sur la passerelle ([exécution local](http://archive.cloudera.com/cdh/3/pig/tutorial.html#Running+the+Pig+Scripts+in+Local+Mode)) avant de le lancer sur le cluster. Avec pyensae, il faut ajouter l'option ``-local`` à la commande ``jobsubmit``.\n",
    "\n",
    "**concaténer les fichiers divisés**\n",
    "\n",
    "Un programme PIG ne produit pas un fichier mais plusieurs fichiers dans un répertoire. La commande [getmerge](http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/FileSystemShell.html) télécharge ces fichiers sur la passerelle et les fusionne en un seul.\n",
    "\n",
    "**ordre des lignes**\n",
    "\n",
    "Les jobs sont distribués, même en faisant rien (LOAD + STORE), il n'est pas garanti que l'ordre des lignes soit préservé. La probabilié que ce soit le cas est quasi nulle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}